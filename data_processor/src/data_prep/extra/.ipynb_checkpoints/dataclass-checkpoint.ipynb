{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21d9499-9e0d-4c00-8ae5-0e7b92591616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/resifis/anaconda3/envs/kaustenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "from skimage import filters\n",
    "import torch.nn as nn\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from models.forecastors.GeneralUnet.general_unet import *\n",
    "from models.forecastors.utils.loss import * \n",
    "import torch.optim as optim\n",
    "from models.utils.configtrain import *\n",
    "from data_prep.config.env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8665036d-2cc3-4d0d-9f05-7a505d5cc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "setup_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de9b0f6-89f9-489d-a56a-9ab70ac77396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "from skimage import filters\n",
    "import torch.nn as nn\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from skimage import filters\n",
    "\n",
    "\n",
    "DATAPATH = os.environ.get(\"DATAPATH\",\"/home/resifis/Desktop/kaustcode/Packages/processed_clean_data\")\n",
    "\n",
    "\n",
    "\n",
    "class Dataset2D():\n",
    "    def __init__(self,solar_type,data_type = \"Train\"):\n",
    "        self.solar_type  = \"DHI\"\n",
    "        self.hcloud = Dataset2D._read_data(\"hcloud\")\n",
    "        self.mcloud = Dataset2D._read_data(\"mcloud\")\n",
    "        self.lcloud = Dataset2D._read_data(\"lcloud\")\n",
    "        self.water_vapor = Dataset2D._read_data(\"water_vapor\")\n",
    "        self.ozone = Dataset2D._read_data(\"ozone\")\n",
    "        self.aerosol = Dataset2D._read_data(\"aerosol\")\n",
    "        self.len = 48222\n",
    "        self.solar_type = solar_type\n",
    "        self.target_data = Dataset2D._read_target(self.solar_type)\n",
    "        self.output = dict()\n",
    "        self.eps = 1e-5\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_data(data_type):\n",
    "        if data_type == \"hcloud\":\n",
    "            hcloud = Dataset(os.path.join(DATAPATH,\"hcloud.nc\"))\n",
    "            return hcloud\n",
    "        elif data_type == \"mcloud\":\n",
    "            mcloud = Dataset(os.path.join(DATAPATH,\"mcloud.nc\"))\n",
    "            return mcloud\n",
    "        elif data_type == \"lcloud\":\n",
    "            lcloud = Dataset(os.path.join(DATAPATH,\"lcloud.nc\"))\n",
    "            return lcloud\n",
    "        elif data_type == \"water_vapor\":\n",
    "            water_vapor = Dataset(os.path.join(DATAPATH,\"water_vapor_new.nc\"))\n",
    "            return water_vapor\n",
    "        elif data_type == \"ozone\":\n",
    "            ozone = Dataset(os.path.join(DATAPATH,\"ozone.nc\"))\n",
    "            return ozone\n",
    "        else:\n",
    "            aerosol = Dataset(os.path.join(DATAPATH,\"aod.nc\"))\n",
    "            return aerosol\n",
    "    @staticmethod\n",
    "    def _read_target(target_type):\n",
    "        if target_type == \"GHI\":\n",
    "            GHI = Dataset(os.path.join(DATAPATH,\"ghi.nc\"))\n",
    "            return GHI\n",
    "        elif target_type == \"DHI\":\n",
    "            DHI = Dataset(os.path.join(DATAPATH,\"dhi.nc\"))\n",
    "            return DHI\n",
    "        else :\n",
    "            DNI = Dataset(os.path.join(DATAPATH,\"dni.nc\"))\n",
    "            return DNI\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _filtering(list_data):\n",
    "        data = list_data.copy()\n",
    "        list_tensors = []\n",
    "        for i in range(len(data)):\n",
    "            arr = filters.sobel(np.array(data[i]))\n",
    "            list_tensors.append(torch.tensor(arr,dtype = torch.float))\n",
    "        return list_tensors\n",
    "    \n",
    "    def _get_tensors(self,item):\n",
    "        all_data = []\n",
    "        data_hcloud =  torch.tensor(self.hcloud.variables[\"cc\"][item,:,:],dtype = torch.float)\n",
    "        data_mcloud =  torch.tensor(self.mcloud.variables[\"cc\"][item,:,:],dtype = torch.float)\n",
    "        data_lcloud =  torch.tensor(self.lcloud.variables[\"cc\"][item,:,:],dtype = torch.float)\n",
    "        data_aerosol = torch.tensor(self.aerosol.variables[\"aod5503d\"][item,:,:],dtype = torch.float)\n",
    "        data_ozone =   torch.tensor(self.ozone.variables[\"o3rad\"][item,:,:],dtype = torch.float)\n",
    "        data_water_vapor = torch.tensor(self.water_vapor.variables[\"qvapor\"][item,:,:],dtype = torch.float)\n",
    "        \n",
    "        all_data.extend([torch.flip(data_hcloud,dims = [0]).unsqueeze(0)/(data_hcloud.max()+self.eps),\n",
    "                         torch.flip(data_mcloud,dims = [0]).unsqueeze(0)/(data_mcloud.max()+self.eps),\n",
    "                         torch.flip(data_lcloud,dims = [0]).unsqueeze(0)/(data_lcloud.max()+self.eps),\n",
    "                         torch.flip(data_aerosol,dims = [0]).unsqueeze(0)/(data_aerosol.max()+self.eps),\n",
    "                         torch.flip(data_ozone,dims = [0]).unsqueeze(0)/(data_ozone.max()+self.eps),\n",
    "                         torch.flip(data_water_vapor,dims = [0]).unsqueeze(0)/(data_water_vapor.max()+self.eps),\n",
    "                        ])\n",
    "        \n",
    "        #filtered = Dataset2D._filtering(all_data)\n",
    "        \n",
    "        #all_data.extend(filtered)\n",
    "        target = torch.tensor(self.target_data.variables[self.solar_type.lower()][item],dtype = torch.float)\n",
    "        target = torch.flip(target,dims = [0])\n",
    "        return all_data,target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        out = dict()\n",
    "        all_data,target = self._get_tensors(item)\n",
    "        out['data'] = torch.cat(all_data,dim = 0)\n",
    "        out['target'] = target/target.max()\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a8b10b-2020-4bf3-8299-fe657c2e270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet = Dataset2D(\"GHI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b59a80-9184-4e56-83f5-010b485857e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 6  \n",
    "output_feature = 1\n",
    "model = UNet2D(in_channels = input_features,\n",
    "               out_channels = output_feature,\n",
    "            )\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m,nn.Conv2d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        m.bias.data.fill_(1)\n",
    "    if isinstance(m,nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        m.bias.data.fill_(1)\n",
    "    if isinstance(m,nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        m.bias.data.fill_(1)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(1)\n",
    "\n",
    "criterion = Loss(\"RMSE\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = criterion.to(device)\n",
    "model = model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor = 0.2,\n",
    "                                                       patience = 3,\n",
    "                                                       verbose = True)\n",
    "\n",
    "shuffle_trainloader = False\n",
    "train_batch_size = 16\n",
    "shuffle_validloader = False\n",
    "valid_batch_size = 16\n",
    "epochs = 100\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48086cf-1b58-4024-bc62-5cad28f78fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import neptune.new as neptune\n",
    "from neptune.new.types import File\n",
    "import torch.nn.init as weight_init\n",
    "from torch.utils.data import RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODELS_WEIGHTS = os.environ.get(\"MODELS_WEIGHTS\",\"/home/resifis/Desktop/kaustcode/Packages/weights2d\")\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class Training():\n",
    "    def __init__(self):\n",
    "        self.aa = 1\n",
    "        \n",
    "\n",
    "        \n",
    "    def _initialize_weights(self,net):\n",
    "        for name, param in net.named_parameters(): \n",
    "            weight_init.normal_(param)\n",
    "        \n",
    "    def train_fn(self,model,train_loader):\n",
    "        model.train()\n",
    "        \n",
    "        tr_loss = 0\n",
    "        counter = 0\n",
    "        losses = AverageMeter()\n",
    "        tqt = tqdm(enumerate(train_loader),total = len(train_loader))\n",
    "        for index,train_batch in tqt:\n",
    "            list_sparse = []\n",
    "            data = train_batch[\"data\"].to(device)\n",
    "            target = train_batch[\"target\"].to(device)\n",
    "            pred_target = model(data)\n",
    "            pred_target = pred_target.squeeze(1)\n",
    "            train_loss = criterion(pred_target,target)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += train_loss.item()        \n",
    "            counter = counter + 1\n",
    "            losses.update(train_loss.item(),pred_target.size(0))\n",
    "            tqt.set_postfix(Loss = losses.avg, Batch_number = index )\n",
    "        return tr_loss/counter\n",
    "\n",
    "    def valid_fn(self,model,validation_loader):\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        counter = 0\n",
    "        losses = AverageMeter()\n",
    "        tqt = tqdm(enumerate(validation_loader),total = len(validation_loader))\n",
    "        with torch.no_grad():\n",
    "            for index, valid_batch in tqt :\n",
    "                list_sparse = []\n",
    "                data = valid_batch[\"data\"].to(device)\n",
    "                target = valid_batch[\"target\"].to(device)\n",
    "                #optimizer.zero_grad()\n",
    "                pred_target = model(data)\n",
    "                pred_target = pred_target.squeeze(1)\n",
    "                validation_loss = criterion(pred_target,target)\n",
    "                val_loss += validation_loss.item()        \n",
    "                counter = counter + 1\n",
    "                losses.update(validation_loss.item(),pred_target.size(0))\n",
    "                tqt.set_postfix(loss = losses.avg, batch_number = index)\n",
    "        return losses.avg\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoints(epoch,model,optimizer,loss):\n",
    "        path = os.path.join(MODELS_WEIGHTS,f\"epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, path)\n",
    "        \n",
    "    def get_dataloader(self,train_dataset,valid_dataset):\n",
    "\n",
    "        train_sampler = RandomSampler(train_dataset,replacement = True,num_samples = 1500)\n",
    "        valid_sampler = RandomSampler(valid_dataset,replacement = True,num_samples = 1000)\n",
    "\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                        shuffle = shuffle_trainloader,\n",
    "                                                        batch_size = train_batch_size,\n",
    "                                                        sampler = train_sampler,\n",
    "                                                       )\n",
    "        valid_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                                        shuffle = shuffle_validloader,\n",
    "                                                        batch_size = valid_batch_size,\n",
    "                                                        sampler = valid_sampler,\n",
    "                                                       )\n",
    "        return train_data_loader,valid_data_loader\n",
    "        \n",
    "    def fit(self,model,train_dataset,valid_dataset):\n",
    "        train_loss = []\n",
    "        valid_loss = []\n",
    "        best = 5000\n",
    "        #self._initialize_weights(model)\n",
    "        model = model.apply(init_weights)\n",
    "        for epoch in range(epochs):\n",
    "            train_data_loader,valid_data_loader = self.get_dataloader(train_dataset,valid_dataset)\n",
    "            if verbose :\n",
    "                print(f\".........EPOCH {epoch}........\")\n",
    "            tr_loss = self.train_fn(model,train_data_loader)\n",
    "            train_loss.append(tr_loss)\n",
    "            if verbose :\n",
    "                print(f\".........Train Loss = {tr_loss}........\")\n",
    "            val_loss = self.valid_fn(model,valid_data_loader)\n",
    "            valid_loss.append(val_loss)\n",
    "            Training.checkpoints(epoch,model,optimizer,val_loss)\n",
    "            scheduler.step(val_loss)initial\n",
    "            if verbose:\n",
    "                print(f\"...........Validation Loss = {val_loss}.......\")\n",
    "\n",
    "            if val_loss < best :\n",
    "                best = val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                print(\"Score is not improving with patient = \",patience)\n",
    "                patience +=1\n",
    "\n",
    "            if patience >= epochs:\n",
    "                print(f\"Early Stopping on Epoch {epoch}\")\n",
    "                print(f\"Best Loss = {best}\")\n",
    "                break\n",
    "                \n",
    "        \n",
    "        PATH = os.path.join(MODELS_WEIGHTS,\"model_2d.pth\")\n",
    "        torch.save(model.state_dict(),PATH)\n",
    "        model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e97a978-bd0c-4450-a075-56861b319e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model bias initial   Parameter containing:\n",
      "tensor([ 0.0222, -0.0062,  0.0366,  0.0068, -0.0202,  0.0180, -0.0076, -0.0323,\n",
      "        -0.0175, -0.0118,  0.0301,  0.0367,  0.0254, -0.0237,  0.0319, -0.0165,\n",
      "        -0.0102, -0.0232,  0.0097,  0.0164,  0.0173,  0.0121,  0.0164,  0.0231,\n",
      "        -0.0361,  0.0182, -0.0256,  0.0051,  0.0108, -0.0095,  0.0328,  0.0123,\n",
      "        -0.0242, -0.0080, -0.0074, -0.0152, -0.0092, -0.0101, -0.0119,  0.0298,\n",
      "         0.0237, -0.0022,  0.0285, -0.0283,  0.0207, -0.0267, -0.0043,  0.0171,\n",
      "        -0.0102,  0.0146, -0.0038,  0.0349,  0.0150, -0.0285, -0.0205,  0.0293,\n",
      "        -0.0387, -0.0060,  0.0193, -0.0317,  0.0057, -0.0403, -0.0104,  0.0122],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "model after intialization Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      ".........EPOCH 0........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 94/94 [00:25<00:00,  3.73it/s, Batch_number=93, Loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Train Loss = 0.4490426480770111........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 63/63 [00:08<00:00,  7.13it/s, batch_number=62, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model bias after 0 epoch    Parameter containing:\n",
      "tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
      "        0.9991], device='cuda:0', requires_grad=True)\n",
      "...........Validation Loss = 0.45152729415893555.......\n",
      ".........EPOCH 1........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████▌       | 44/94 [00:09<00:10,  4.67it/s, Batch_number=43, Loss=0.453]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m job \u001b[38;5;241m=\u001b[39m Training()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDataSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDataSet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mTraining.fit\u001b[0;34m(self, model, train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose :\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.........EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m........\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose :\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mTraining.train_fn\u001b[0;34m(self, model, train_loader)\u001b[0m\n\u001b[1;32m     57\u001b[0m losses \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[1;32m     58\u001b[0m tqt \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader),total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,train_batch \u001b[38;5;129;01min\u001b[39;00m tqt:\n\u001b[1;32m     60\u001b[0m     list_sparse \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m     data \u001b[38;5;241m=\u001b[39m train_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaustenv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaustenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/kaustenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaustenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/kaustenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mDataset2D.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,item):\n\u001b[1;32m    105\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 106\u001b[0m     all_data,target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_data,dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    108\u001b[0m     out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m/\u001b[39mtarget\u001b[38;5;241m.\u001b[39mmax()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mDataset2D._get_tensors\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     82\u001b[0m data_ozone \u001b[38;5;241m=\u001b[39m   torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mozone\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo3rad\u001b[39m\u001b[38;5;124m\"\u001b[39m][item,:,:],dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     83\u001b[0m data_water_vapor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwater_vapor\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqvapor\u001b[39m\u001b[38;5;124m\"\u001b[39m][item,:,:],dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     85\u001b[0m all_data\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mflip(data_hcloud,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(data_hcloud\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[0;32m---> 86\u001b[0m                  torch\u001b[38;5;241m.\u001b[39mflip(data_mcloud,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[43mdata_mcloud\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[1;32m     87\u001b[0m                  torch\u001b[38;5;241m.\u001b[39mflip(data_lcloud,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(data_lcloud\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[1;32m     88\u001b[0m                  torch\u001b[38;5;241m.\u001b[39mflip(data_aerosol,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(data_aerosol\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[1;32m     89\u001b[0m                  torch\u001b[38;5;241m.\u001b[39mflip(data_ozone,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(data_ozone\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[1;32m     90\u001b[0m                  torch\u001b[38;5;241m.\u001b[39mflip(data_water_vapor,dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m(data_water_vapor\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps),\n\u001b[1;32m     91\u001b[0m                 ])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#filtered = Dataset2D._filtering(all_data)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#all_data.extend(filtered)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_data\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolar_type\u001b[38;5;241m.\u001b[39mlower()][item],dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job = Training()\n",
    "job.fit(model,DataSet,DataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9926ea8-37da-4e82-add1-75825a3216aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = DataSet[0]['data'].unsqueeze(0)\n",
    "input_ = input_.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1badbd6-02b5-4ab1-b8f9-08ed831c98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022f37b-49b8-4232-b838-2e5cd0f55067",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab28921-9678-48c6-9edc-a4f734f91287",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6651cd-ed8b-4c29-a540-e246c206eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994846a-0eaf-4d80-8df7-be5222f478ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40ecf3-ca15-48ad-b378-5bdf8040f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2fdced-a126-4655-8d05-389e1682d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self,loss_name):\n",
    "        super(Loss,self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.l1  = nn.L1Loss()\n",
    "        self.loss_name = loss_name\n",
    "        self.psnr = PSNR()\n",
    "        self.ssim = SSIM3D(window_size = 11)\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        if self.loss_name == \"RMSE\":\n",
    "            return torch.sqrt(self.mse(yhat,y))\n",
    "        elif self.loss_name == \"MSE\":\n",
    "            return self.mse(yhat,y)\n",
    "        elif self.loss_name == \"L1Loss\":\n",
    "            return self.l1(yhat,y)\n",
    "        elif self.loss_name == \"PSNR\":\n",
    "            return self.psnr(yhat,y)\n",
    "        elif self.loss_name == \"SSIM\":\n",
    "            return 1-self.ssim(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2caa9d-870f-4178-a49d-b0a79bbf877b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff4a56-5665-40f5-8687-9a06033f9189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
