{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b8de85-cd0b-4ab8-aca3-781800f7f78a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecastors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mConv3DUNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecastors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforecastors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models.forecastors.Conv3DUNet.unet import *\n",
    "from models.forecastors.utils.engine import *\n",
    "from models.forecastors.utils.utils import *\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from models.forecastors.utils.loss import * \n",
    "from data_prep.dataset.wrf_data import *\n",
    "from models.utils.configtrain import *\n",
    "from data_prep.config.env import *\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import torch.optim as optim\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from skimage import filters\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb2f33-49cb-4af7-9f98-48feaee7a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True \n",
    "setup_seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18767593-90c6-46b7-9b51-2a597fb3730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = os.environ.get(\"DATAPATH\",\"/home/resifis/Desktop/kaustcode/Packages/processed_clean_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e78a1-dd98-495d-921b-74975063aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "from netCDF4 import Dataset\n",
    "from skimage import filters\n",
    "import torch.nn as nn\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "DATAPATH = os.environ.get(\"DATAPATH\",\"/home/resifis/Desktop/kaustcode/Packages/processed_clean_data\")\n",
    "\n",
    "\n",
    "class WRFDataset():\n",
    "    \"\"\" This Class is an iterable Dataset class which will be needed by the dataloader\n",
    "        to get the batch data\n",
    "        window_size: The length of time window that we are going to take\n",
    "        solar_type: The type of solar irradiance that we are going to predict (GHI, DHI, DNI)\n",
    "        data_type: The type of the dataset it can be training dataset or a validation dataset\n",
    "        domaine_size: Downscalling the domaine (lat,lon)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: int = 10,\n",
    "        solar_type: str = \"GHI\",\n",
    "        data_type: str = \"train\",\n",
    "        domaine_size = \"reduced\",\n",
    "    ):\n",
    "        self.hcloud = WRFDataset._read_data(\"hcloud\")\n",
    "        self.mcloud = WRFDataset._read_data(\"mcloud\")\n",
    "        self.lcloud = WRFDataset._read_data(\"lcloud\")\n",
    "        self.water_vapor = WRFDataset._read_data(\"water_vapor\")\n",
    "        self.ozone = WRFDataset._read_data(\"ozone\")\n",
    "        self.aerosol = WRFDataset._read_data(\"aod\")\n",
    "        \n",
    "        self.t2 = WRFDataset._read_data(\"t2\")\n",
    "        self.td2 = WRFDataset._read_data(\"td2\")\n",
    "        self.mslp = WRFDataset._read_data(\"mslp\")\n",
    "        self.rain = WRFDataset._read_data(\"rain\")\n",
    "        self.ws = WRFDataset._read_data(\"WS\")\n",
    "        self.wd = WRFDataset._read_data(\"WD\")\n",
    "        self.thick = WRFDataset._read_data(\"thickness\")\n",
    "        \n",
    "        self.solar_type = solar_type\n",
    "        self.target_data = WRFDataset._read_target(self.solar_type)\n",
    "        self.window_size = window_size\n",
    "        self.data_type = data_type\n",
    "        self.domaine_size = domaine_size\n",
    "        if self.domaine_size == \"reduced\":\n",
    "            self.idx_row = [i for i in range(int(213/2)) if i%1 == 0]\n",
    "            self.idx_col = [i for i in range(int(288/2),288) if i%1 == 0]\n",
    "        else:\n",
    "            self.idx_row = [i for i in range(213) if i%1 != 0]\n",
    "            self.idx_col = [i for i in range(288) if i%1 != 0]\n",
    "            \n",
    "        self.output = dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_data(data_type):\n",
    "        if data_type == \"hcloud\":\n",
    "            hcloud = Dataset(os.path.join(DATAPATH,\"hcloud.nc\"))\n",
    "            return hcloud\n",
    "        elif data_type == \"mcloud\":\n",
    "            mcloud = Dataset(os.path.join(DATAPATH,\"mcloud.nc\"))\n",
    "            return mcloud\n",
    "        elif data_type == \"lcloud\":\n",
    "            lcloud = Dataset(os.path.join(DATAPATH,\"lcloud.nc\"))\n",
    "            return lcloud\n",
    "        elif data_type == \"water_vapor\":\n",
    "            water_vapor = Dataset(os.path.join(DATAPATH,\"water_vapor_new.nc\"))\n",
    "            return water_vapor\n",
    "        elif data_type == \"ozone\":\n",
    "            ozone = Dataset(os.path.join(DATAPATH,\"ozone.nc\"))\n",
    "            return ozone\n",
    "        elif data_type == \"aod\":\n",
    "            aerosol = Dataset(os.path.join(DATAPATH,\"aod.nc\"))\n",
    "            return aerosol\n",
    "        elif data_type == 't2':\n",
    "            t2 = Dataset(os.path.join(DATAPATH,\"t2.nc\"))\n",
    "            return t2\n",
    "        elif data_type == 'td2':\n",
    "            td2 = Dataset(os.path.join(DATAPATH,\"dt2.nc\"))\n",
    "            return td2\n",
    "        elif data_type == \"mslp\":\n",
    "            mslp = Dataset(os.path.join(DATAPATH,\"mslp.nc\"))\n",
    "            return mslp\n",
    "        elif data_type == 'WS':\n",
    "            ws = Dataset(os.path.join(DATAPATH,\"WS.nc\"))\n",
    "            return ws\n",
    "        elif data_type == \"WD\":\n",
    "            wd = Dataset(os.path.join(DATAPATH,\"WD.nc\"))\n",
    "            return wd \n",
    "        elif data_type == \"rain\":\n",
    "            rain  = Dataset(os.path.join(DATAPATH,\"rain.nc\"))\n",
    "            return rain\n",
    "            \n",
    "        elif data_type == 'thickness':\n",
    "            thickness = Dataset(os.path.join(DATAPATH,\"thickness.nc\"))\n",
    "            return thickness\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_target(target_type):\n",
    "        if target_type == \"GHI\":\n",
    "            GHI = Dataset(os.path.join(DATAPATH,\"ghi.nc\"))\n",
    "            return GHI\n",
    "        elif target_type == \"DHI\":\n",
    "            DHI = Dataset(os.path.join(DATAPATH,\"dhi.nc\"))\n",
    "            return DHI\n",
    "        else :\n",
    "            DNI = Dataset(os.path.join(DATAPATH,\"dni.nc\"))\n",
    "            return DNI\n",
    "        \n",
    "    @staticmethod\n",
    "    def _normalize(tensor):\n",
    "        normalized_tensor = nn.functional.normalize(tensor,dim = 0)\n",
    "        return tensor\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _filtering(data):\n",
    "        arr_data = data.copy()\n",
    "        list_tensors = []\n",
    "        for i in range(arr_data.shape[0]):\n",
    "            arr = filters.sobel(np.flip(arr_data[i],axis = 0))\n",
    "            list_tensors.append(torch.tensor(arr,dtype = torch.float).unsqueeze(0))\n",
    "        filtered_data = torch.cat(list_tensors,dim = 0)\n",
    "        filtered_data = filtered_data.unsqueeze(0)\n",
    "        return filtered_data\n",
    "    \n",
    "    def _scaling(self,reshaped_data):\n",
    "        scaled_data = []\n",
    "        for data in reshaped_data:\n",
    "            list_steps = []\n",
    "            for step in range(self.window_size):\n",
    "                step_data = torch.tensor(data[step])\n",
    "                max_ = step_data.max()\n",
    "                list_steps.append((step_data/(max_+ 1e-5)).unsqueeze(0))\n",
    "            steps_torch = torch.cat(list_steps,dim = 0)\n",
    "            scaled_data.append(np.array(steps_torch))\n",
    "        return scaled_data\n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "    def _get_tensor_data(self,item):\n",
    "        raw_data = list()\n",
    "        reshaped_data = list()\n",
    "        all_data = list()\n",
    "        data_hcloud =  1e-1 * np.array(self.hcloud.variables[\"cc\"][item:item+self.window_size,:,:])\n",
    "        data_mcloud =  1e-1*np.array(self.mcloud.variables[\"cc\"][item:item+self.window_size,:,:])\n",
    "        data_lcloud =  1e-1*np.array(self.lcloud.variables[\"cc\"][item:item+self.window_size,:,:])\n",
    "        data_aerosol = 100*np.array(self.aerosol.variables[\"aod5503d\"][item:item+self.window_size,:,:])\n",
    "        data_ozone =   1e7*np.array(self.ozone.variables[\"o3rad\"][item:item+self.window_size,:,:])\n",
    "        data_water_vapor = 1e3*np.array(self.water_vapor.variables[\"qvapor\"][item:item+self.window_size,:,:])\n",
    "        t2 = 1e-1*np.array(self.t2.variables['t2'][item:item+self.window_size,:,:])\n",
    "        td2 = 1e-1*np.array(self.td2.variables['td2'][item:item+self.window_size,:,:])\n",
    "        mslp = 1e-3 * np.array(self.mslp.variables['mslp'][item:item+self.window_size,:,:])\n",
    "        rain = 1e5 * np.array(self.rain.variables['rain'][item:item+self.window_size,:,:])\n",
    "        ws = np.array(self.ws.variables['WS'][item:item+self.window_size,:,:])\n",
    "        wd = np.array(self.wd.variables['WD'][item:item+self.window_size,:,:])\n",
    "        thick = np.array(self.thick.variables['thickness'][item:item+self.window_size,:,:])\n",
    "        \n",
    "        \n",
    "        raw_data.extend([data_hcloud,\n",
    "                         data_mcloud,\n",
    "                         data_lcloud,\n",
    "                         thick,\n",
    "                         data_aerosol,\n",
    "                         data_ozone,\n",
    "                         data_water_vapor,\n",
    "                         t2,\n",
    "                         td2,\n",
    "                         mslp,\n",
    "                         rain,\n",
    "                         ws,\n",
    "                         wd,\n",
    "                         ])\n",
    "        \n",
    "        for data in raw_data : \n",
    "            shrink_data = np.delete(data,self.idx_row,axis = 1)\n",
    "            shrink_data = np.delete(shrink_data,self.idx_col,axis = 2)\n",
    "            reshaped_data.append(shrink_data)\n",
    "            \n",
    "        reshaped_scaled_data = self._scaling(reshaped_data)\n",
    "        \n",
    "    \n",
    "        for data in reshaped_scaled_data:\n",
    "            all_data.append(WRFDataset._normalize(torch.flip(torch.tensor(data),dims = [1])).unsqueeze(0))\n",
    "            \n",
    "        for data in reshaped_scaled_data:\n",
    "            all_data.append(WRFDataset._filtering(data))\n",
    "        \n",
    "        target_cdf = np.array(self.target_data.variables[self.solar_type.lower()][item:item+self.window_size,:,:])\n",
    "        target_cdf = np.delete(target_cdf,self.idx_row,axis = 1)\n",
    "        target_cdf = np.delete(target_cdf,self.idx_col,axis = 2)\n",
    "        target_cdf = torch.tensor(target_cdf)\n",
    "        target = target_cdf.squeeze(1)\n",
    "        target = torch.flip(target,dims = [1])\n",
    "        return all_data,target\n",
    "        \n",
    "    def __len__(self):\n",
    "        full_len = self.hcloud.variables[\"cc\"].shape[0]\n",
    "        if self.data_type == \"train\":\n",
    "            data_len = full_len - int(0.1*full_len)\n",
    "        else:\n",
    "            data_len = int(0.1*full_len)\n",
    "        return data_len  - self.window_size\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        if self.data_type == \"valid\":\n",
    "            item = item + int(0.9*self.hcloud.variables[\"cc\"].shape[0])\n",
    "        all_data,target = self._get_tensor_data(item)\n",
    "        self.output[\"data\"] = torch.cat(all_data,dim = 0)\n",
    "        self.output[\"target\"] = target\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083dfcc-71b6-46b9-b32e-94cb1ee040a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "window_size = 13\n",
    "WRFData = WRFDataset(window_size = window_size,solar_type= \"DNI\",domaine_size = \"reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748226d-cd11-4be3-b1da-7dfff204402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(WRFData[item]['data'][13][7])\n",
    "cax = plt.axes([0.85, 0.1, 0.075, 0.8])\n",
    "plt.colorbar(cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af7616-566c-4c7c-bb4c-091e0b94c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRFData[item]['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad94b75-335d-4e0b-be26-466cbdef736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(WRFData[item]['data'][0][7])\n",
    "cax = plt.axes([0.85, 0.1, 0.075, 0.8])\n",
    "plt.colorbar(cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c03940-2b4e-482a-bd9f-3b4e950596a4",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b20345-a98c-4499-a33a-886d1cd79f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(WRFData[item]['target'][2])\n",
    "cax = plt.axes([0.85, 0.1, 0.075, 0.8])\n",
    "plt.colorbar(cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3110-57dc-49c3-8501-2fef565b3b74",
   "metadata": {},
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbd0634-ded6-4f9e-a48e-ec31188d0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 4\n",
    "output_feature = 1\n",
    "model = UNet(n_channels = input_features,\n",
    "             n_classes = output_feature,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eedaa8d-c088-4b9f-856e-4b717afbf968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m,nn.Conv3d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "    if isinstance(m,nn.ConvTranspose3d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "    if isinstance(m,nn.BatchNorm3d):\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "model = model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c23615-b054-4e35-8322-5249a1c54e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Loss(\"RMSE\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor = 0.2,\n",
    "                                                       patience = 3,\n",
    "                                                       verbose = True)\n",
    "\n",
    "shuffle_trainloader = False\n",
    "train_batch_size = 3\n",
    "shuffle_validloader = False\n",
    "valid_batch_size = 3\n",
    "epoch = 100\n",
    "verbose = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a89f34-ebd5-45ec-8cd7-5470bc1e04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainingConfig(model,\n",
    "                              criterion,\n",
    "                              optimizer,\n",
    "                              scheduler,\n",
    "                              device,\n",
    "                              shuffle_trainloader,\n",
    "                              train_batch_size,\n",
    "                              shuffle_validloader,\n",
    "                              valid_batch_size,\n",
    "                              epoch,\n",
    "                              verbose,\n",
    "                             )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b34ba25-f0d4-4194-82dc-9e463679f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........EPOCH 0........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████████▎                                                                                       | 46/167 [01:18<03:25,  1.70s/it, Batch_number=45, Loss=846, Training_Accuracy=0.082]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = Training(train_config)\n",
    "job.fit(WRFData,WRFData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65560a9-5a01-4266-ae9c-18df14d7ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = WRFData[5]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7c9b0-ea05-4719-9f22-d74965bf637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d9fc6-2a94-4262-a8a4-ac4743c8e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(target[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c566e-e697-4f83-86f6-d003e0cebd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(WRFData[9]['data'].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cb2c3-64f0-4a0d-bd81-118b383bdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prediction.squeeze(0).squeeze(0)[16].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03dc4e-5132-49a9-9298-991ea6e703cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.squeeze(0).squeeze(0)[0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d98e1-d5fc-44ec-9b2e-f8ec3ceff59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace13569-fe44-4436-914f-543e64fe6256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
