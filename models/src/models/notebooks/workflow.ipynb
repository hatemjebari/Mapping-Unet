{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90509bcf-42b8-4487-ba61-8b26643c41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention module \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)\n",
    "        output, attn = self.attention(q, k, v)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" A two-feed-forward-layer module \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Conv1d(d_in, d_hid, 1)\n",
    "        self.w_2 = nn.Conv1d(d_hid, d_in, 1)\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\" Compose with two layers \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_inner=1024, n_head=8, d_k=64, d_v=64, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(enc_input, enc_input, enc_input)\n",
    "        \n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "\n",
    "        return enc_output, enc_slf_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cc2786-a435-427e-a4cb-044b505650fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = torch.randn(100,10,14)\n",
    "encode = EncoderLayer(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cece9c06-7cc7-4677-9af5-d76806d35493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0406, -1.2483,  0.6753,  ...,  0.1709,  1.5389,  1.2648],\n",
       "         [ 0.9159, -2.2183, -1.2717,  ..., -0.9412,  1.7283, -0.2817],\n",
       "         [-1.1294,  1.2231,  1.2109,  ..., -0.8877, -0.1374,  0.4508],\n",
       "         ...,\n",
       "         [-0.1608, -0.8997,  0.1449,  ..., -0.4128,  1.5438, -1.1066],\n",
       "         [-0.0599,  0.4641, -1.9637,  ..., -0.3171, -1.6015,  1.0451],\n",
       "         [ 0.1465,  0.8424, -0.5878,  ..., -0.3073,  1.2185, -0.3230]],\n",
       "\n",
       "        [[ 0.0344, -0.5854, -0.1213,  ...,  0.7590, -1.0364,  1.3861],\n",
       "         [ 0.7507, -0.2037,  0.8846,  ..., -1.0601,  0.9511,  1.7958],\n",
       "         [-0.5900,  0.1403,  0.7025,  ...,  0.0994,  0.0293, -1.1251],\n",
       "         ...,\n",
       "         [ 0.1602, -1.3555,  0.8423,  ...,  1.5417,  0.3075, -0.8635],\n",
       "         [-0.6263, -0.0900,  1.9712,  ..., -0.4856,  0.6477, -1.3915],\n",
       "         [-1.9873, -0.2725, -0.2912,  ..., -0.4379, -0.5548, -1.1041]],\n",
       "\n",
       "        [[-0.3450,  0.4920, -0.0444,  ..., -1.7171,  1.1074, -0.7276],\n",
       "         [ 1.2170, -0.6998, -0.1333,  ..., -0.9973, -0.5526,  0.3822],\n",
       "         [-0.2437, -1.5204,  1.1532,  ..., -1.1775, -0.5028, -0.4579],\n",
       "         ...,\n",
       "         [ 0.5623,  0.9687, -0.6330,  ..., -1.9979,  0.8606, -0.6500],\n",
       "         [ 1.2924, -1.1403,  0.1158,  ..., -1.7808, -0.0690,  0.1077],\n",
       "         [ 1.7797, -0.8199, -1.6035,  ...,  0.4772,  0.3949,  0.5823]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2356, -0.9232, -0.9660,  ..., -0.0812,  1.1167, -0.7858],\n",
       "         [-1.9016,  0.4782, -0.4204,  ..., -1.7611,  1.7891, -0.7320],\n",
       "         [ 0.3588, -0.8264, -0.5353,  ...,  0.0174,  1.5219,  0.3525],\n",
       "         ...,\n",
       "         [-0.3953,  0.8330, -0.9892,  ..., -2.1227,  0.3765, -1.3552],\n",
       "         [-1.1045, -0.2353,  0.3795,  ...,  0.5715,  2.5862, -0.4598],\n",
       "         [ 0.4836,  0.7237,  0.8466,  ..., -1.0153, -0.3837,  1.5906]],\n",
       "\n",
       "        [[-0.7180, -0.8074,  0.3766,  ..., -1.2823,  1.0148, -0.1845],\n",
       "         [-1.6614,  1.1055, -1.2375,  ...,  0.9392,  0.5011, -0.4993],\n",
       "         [-1.9982, -0.6990, -0.1531,  ..., -0.2641,  1.7149, -0.4725],\n",
       "         ...,\n",
       "         [ 1.1387, -0.5167, -0.6256,  ...,  0.0248,  2.2016,  1.1067],\n",
       "         [-1.7533,  0.5007,  0.9332,  ..., -1.9580, -0.0700,  0.3901],\n",
       "         [ 0.5725,  0.9715,  0.8521,  ...,  0.0106,  0.0320, -0.5218]],\n",
       "\n",
       "        [[-0.0597, -0.1641, -0.0369,  ...,  1.3182, -0.5681, -0.6882],\n",
       "         [ 1.2891, -2.0731,  1.1462,  ..., -1.1661,  0.5806, -0.1547],\n",
       "         [ 1.0362, -1.4265,  0.7637,  ..., -1.0822,  1.5292, -0.5255],\n",
       "         ...,\n",
       "         [-0.6224, -0.6287, -0.6744,  ..., -0.0116,  0.3183, -1.2060],\n",
       "         [ 1.5879, -0.7224,  0.4635,  ..., -0.8967, -1.1025, -1.0071],\n",
       "         [-0.4660,  0.9677,  1.3031,  ...,  0.7513,  0.1919,  0.7996]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(tens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6948c4-fa8f-4ebd-87ef-6e8c5fd8017c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
